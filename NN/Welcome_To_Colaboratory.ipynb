{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\">\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "metadata": {
        "id": "BwJnT0bKfG1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5596
        },
        "outputId": "9e305908-222b-4f47-f26e-e4eacf2c2923"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rd\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "# Preparing the data set\n",
        "with open('AI_quick_draw.pickle', 'rb') as open_ai_quick:\n",
        "    data_train = pickle.load(open_ai_quick)\n",
        "    label_train1 = pickle.load(open_ai_quick)\n",
        "    data_test = pickle.load(open_ai_quick)\n",
        "    label_test1 = pickle.load(open_ai_quick)\n",
        "n_classes = len(np.unique(label_train1))\n",
        "# convert labels to 0-1 hot encoding\n",
        "label_train = np.zeros((label_train1.shape[0], n_classes))\n",
        "a = np.arange(label_train1.shape[0], dtype=np.int64)\n",
        "b = np.array(label_train1, dtype=np.int64).reshape((label_train1.shape[0],))\n",
        "label_train[a, b] = 1\n",
        "\n",
        "label_test = np.zeros((label_test1.shape[0], n_classes))\n",
        "c = np.arange(label_test1.shape[0], dtype=np.int64)\n",
        "d = np.array(label_test1, dtype=np.int64).reshape((label_test1.shape[0],))\n",
        "label_test[c, d] = 1\n",
        "\n",
        "\n",
        "begin_time = time.time()\n",
        "\n",
        "# Network parameters\n",
        "learning_rate = 0.002\n",
        "batch_size = 200\n",
        "training_iteration_num = 3000\n",
        "\n",
        "n_hidden_1 = 256  # 1st layer number of neurons\n",
        "n_hidden_2 = 256  # 2nd layer number of neurons\n",
        "n_input = data_train.shape[1]  # MNIST data input (img shape: 28*28)\n",
        "n_classes = label_train.shape[1]  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5\n",
        "\n",
        "# Deep ANN model\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def multilayer_perceptron(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "\n",
        "    # Hidden fully connected layer with 128 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    # layer_2 = tf.nn.dropout(layer_2, dropout)\n",
        "\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #with tf.device('/gpu:1'):\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    total_batch_train = int(data_train.shape[0] / batch_size)\n",
        "    total_batch_test = int(data_test.shape[0] / batch_size)\n",
        "\n",
        "    for iter_num in range(training_iteration_num):\n",
        "        avg_cost_test = 0.\n",
        "        avg_acc_test = 0.\n",
        "        select=rd.sample(range(data_train.shape[0]), batch_size)\n",
        "        train_x = data_train[select,:]\n",
        "        train_y = label_train[select,:]\n",
        "\n",
        "        _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
        "\n",
        "        _label_train = [np.argmax(i) for i in _logits_train]\n",
        "        _label_train_y = [np.argmax(i) for i in train_y]\n",
        "        _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
        "\n",
        "\n",
        "        for i in range(total_batch_test):\n",
        "            test_x = data_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            test_y = label_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
        "            avg_cost_test += c_test\n",
        "            _label_test = [np.argmax(i) for i in _logits_test]\n",
        "            _label_test_y = [np.argmax(i) for i in test_y]\n",
        "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
        "            avg_acc_test += _accuracy_test\n",
        "        if iter_num % 5 == 0:\n",
        "            print(\"[%5d/%d] train_loss: %.5f, train_accuracy: %.5f ; test_loss: %.5f, test_accuracy: %.5f\" %\n",
        "                  (iter_num, training_iteration_num, c_train, _accuracy_train, avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "            #print(\"Test Loss: %f, Test_acc: %f\" % (avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "\n",
        "end_time = time.time()\n",
        "complete = end_time - begin_time\n",
        "min = int(complete/60)\n",
        "secs = round(complete % 60, 2)\n",
        "\n",
        "print(\"Your program finished in %d minutes %d seconds!\" % (min, secs))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-7a5f275ca207>:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "[    0/3000] train_loss: 13.06543, train_accuracy: 0.07000 ; test_loss: 12.20326, test_accuracy: 0.09044\n",
            "[    5/3000] train_loss: 8.85585, train_accuracy: 0.10500 ; test_loss: 9.11238, test_accuracy: 0.09912\n",
            "[   10/3000] train_loss: 8.51222, train_accuracy: 0.11000 ; test_loss: 8.36787, test_accuracy: 0.10968\n",
            "[   15/3000] train_loss: 8.01654, train_accuracy: 0.13000 ; test_loss: 8.07394, test_accuracy: 0.11708\n",
            "[   20/3000] train_loss: 7.93693, train_accuracy: 0.09500 ; test_loss: 7.63466, test_accuracy: 0.12940\n",
            "[   25/3000] train_loss: 6.91922, train_accuracy: 0.16000 ; test_loss: 7.13656, test_accuracy: 0.13836\n",
            "[   30/3000] train_loss: 6.95389, train_accuracy: 0.14000 ; test_loss: 6.68051, test_accuracy: 0.15144\n",
            "[   35/3000] train_loss: 6.19015, train_accuracy: 0.17500 ; test_loss: 6.40392, test_accuracy: 0.15980\n",
            "[   40/3000] train_loss: 5.81665, train_accuracy: 0.15500 ; test_loss: 6.17777, test_accuracy: 0.17060\n",
            "[   45/3000] train_loss: 5.68159, train_accuracy: 0.23500 ; test_loss: 5.94271, test_accuracy: 0.18224\n",
            "[   50/3000] train_loss: 5.31382, train_accuracy: 0.25000 ; test_loss: 5.69671, test_accuracy: 0.19420\n",
            "[   55/3000] train_loss: 6.11911, train_accuracy: 0.18000 ; test_loss: 5.54046, test_accuracy: 0.19736\n",
            "[   60/3000] train_loss: 5.47926, train_accuracy: 0.18500 ; test_loss: 5.38016, test_accuracy: 0.20664\n",
            "[   65/3000] train_loss: 5.23592, train_accuracy: 0.21000 ; test_loss: 5.18673, test_accuracy: 0.21544\n",
            "[   70/3000] train_loss: 4.74299, train_accuracy: 0.25500 ; test_loss: 5.04009, test_accuracy: 0.22484\n",
            "[   75/3000] train_loss: 5.23779, train_accuracy: 0.22000 ; test_loss: 4.88866, test_accuracy: 0.23344\n",
            "[   80/3000] train_loss: 5.10800, train_accuracy: 0.22500 ; test_loss: 4.75405, test_accuracy: 0.24232\n",
            "[   85/3000] train_loss: 4.28643, train_accuracy: 0.21500 ; test_loss: 4.63075, test_accuracy: 0.25032\n",
            "[   90/3000] train_loss: 4.33037, train_accuracy: 0.27000 ; test_loss: 4.53268, test_accuracy: 0.25684\n",
            "[   95/3000] train_loss: 4.63227, train_accuracy: 0.24500 ; test_loss: 4.39581, test_accuracy: 0.26308\n",
            "[  100/3000] train_loss: 4.23386, train_accuracy: 0.27500 ; test_loss: 4.29442, test_accuracy: 0.26608\n",
            "[  105/3000] train_loss: 4.08430, train_accuracy: 0.29500 ; test_loss: 4.19534, test_accuracy: 0.26932\n",
            "[  110/3000] train_loss: 4.23701, train_accuracy: 0.25000 ; test_loss: 4.07420, test_accuracy: 0.28108\n",
            "[  115/3000] train_loss: 3.65002, train_accuracy: 0.29000 ; test_loss: 4.00136, test_accuracy: 0.28596\n",
            "[  120/3000] train_loss: 3.97119, train_accuracy: 0.29500 ; test_loss: 3.91844, test_accuracy: 0.29052\n",
            "[  125/3000] train_loss: 3.65263, train_accuracy: 0.31000 ; test_loss: 3.83507, test_accuracy: 0.29116\n",
            "[  130/3000] train_loss: 3.57408, train_accuracy: 0.33500 ; test_loss: 3.73886, test_accuracy: 0.29508\n",
            "[  135/3000] train_loss: 3.52829, train_accuracy: 0.32000 ; test_loss: 3.67243, test_accuracy: 0.29884\n",
            "[  140/3000] train_loss: 3.99186, train_accuracy: 0.23500 ; test_loss: 3.60920, test_accuracy: 0.30236\n",
            "[  145/3000] train_loss: 3.20402, train_accuracy: 0.35000 ; test_loss: 3.53955, test_accuracy: 0.30672\n",
            "[  150/3000] train_loss: 3.51066, train_accuracy: 0.31500 ; test_loss: 3.48585, test_accuracy: 0.30916\n",
            "[  155/3000] train_loss: 3.44250, train_accuracy: 0.27500 ; test_loss: 3.41940, test_accuracy: 0.31148\n",
            "[  160/3000] train_loss: 3.31762, train_accuracy: 0.35500 ; test_loss: 3.34980, test_accuracy: 0.31740\n",
            "[  165/3000] train_loss: 2.98153, train_accuracy: 0.41500 ; test_loss: 3.30128, test_accuracy: 0.32300\n",
            "[  170/3000] train_loss: 3.28192, train_accuracy: 0.31500 ; test_loss: 3.25644, test_accuracy: 0.32500\n",
            "[  175/3000] train_loss: 3.19628, train_accuracy: 0.33000 ; test_loss: 3.20224, test_accuracy: 0.32652\n",
            "[  180/3000] train_loss: 3.24202, train_accuracy: 0.34000 ; test_loss: 3.15471, test_accuracy: 0.33012\n",
            "[  185/3000] train_loss: 3.04435, train_accuracy: 0.35500 ; test_loss: 3.10852, test_accuracy: 0.33580\n",
            "[  190/3000] train_loss: 2.98166, train_accuracy: 0.35500 ; test_loss: 3.06405, test_accuracy: 0.33808\n",
            "[  195/3000] train_loss: 3.19559, train_accuracy: 0.36500 ; test_loss: 3.02005, test_accuracy: 0.33992\n",
            "[  200/3000] train_loss: 3.42935, train_accuracy: 0.31000 ; test_loss: 2.97604, test_accuracy: 0.34128\n",
            "[  205/3000] train_loss: 3.16378, train_accuracy: 0.35000 ; test_loss: 2.94091, test_accuracy: 0.34628\n",
            "[  210/3000] train_loss: 2.60310, train_accuracy: 0.44000 ; test_loss: 2.89007, test_accuracy: 0.34728\n",
            "[  215/3000] train_loss: 2.76579, train_accuracy: 0.35500 ; test_loss: 2.85053, test_accuracy: 0.35064\n",
            "[  220/3000] train_loss: 2.84913, train_accuracy: 0.36000 ; test_loss: 2.81550, test_accuracy: 0.35136\n",
            "[  225/3000] train_loss: 2.78914, train_accuracy: 0.40500 ; test_loss: 2.78732, test_accuracy: 0.35768\n",
            "[  230/3000] train_loss: 2.58573, train_accuracy: 0.35000 ; test_loss: 2.74611, test_accuracy: 0.35800\n",
            "[  235/3000] train_loss: 2.65089, train_accuracy: 0.37500 ; test_loss: 2.70040, test_accuracy: 0.35516\n",
            "[  240/3000] train_loss: 2.51409, train_accuracy: 0.39500 ; test_loss: 2.67252, test_accuracy: 0.35856\n",
            "[  245/3000] train_loss: 2.37582, train_accuracy: 0.40000 ; test_loss: 2.63577, test_accuracy: 0.36340\n",
            "[  250/3000] train_loss: 2.46045, train_accuracy: 0.37500 ; test_loss: 2.61189, test_accuracy: 0.36572\n",
            "[  255/3000] train_loss: 2.76717, train_accuracy: 0.32500 ; test_loss: 2.58026, test_accuracy: 0.36436\n",
            "[  260/3000] train_loss: 2.70673, train_accuracy: 0.34000 ; test_loss: 2.55745, test_accuracy: 0.36508\n",
            "[  265/3000] train_loss: 2.79298, train_accuracy: 0.35500 ; test_loss: 2.53035, test_accuracy: 0.36584\n",
            "[  270/3000] train_loss: 2.55600, train_accuracy: 0.38000 ; test_loss: 2.50389, test_accuracy: 0.36820\n",
            "[  275/3000] train_loss: 2.54868, train_accuracy: 0.37000 ; test_loss: 2.48050, test_accuracy: 0.36836\n",
            "[  280/3000] train_loss: 2.25643, train_accuracy: 0.35500 ; test_loss: 2.45978, test_accuracy: 0.37200\n",
            "[  285/3000] train_loss: 2.33201, train_accuracy: 0.38500 ; test_loss: 2.43690, test_accuracy: 0.37344\n",
            "[  290/3000] train_loss: 2.38135, train_accuracy: 0.41500 ; test_loss: 2.42087, test_accuracy: 0.37792\n",
            "[  295/3000] train_loss: 2.42302, train_accuracy: 0.38000 ; test_loss: 2.40217, test_accuracy: 0.37860\n",
            "[  300/3000] train_loss: 2.27400, train_accuracy: 0.41000 ; test_loss: 2.37906, test_accuracy: 0.37884\n",
            "[  305/3000] train_loss: 2.38223, train_accuracy: 0.36500 ; test_loss: 2.35495, test_accuracy: 0.38072\n",
            "[  310/3000] train_loss: 2.45937, train_accuracy: 0.33000 ; test_loss: 2.33768, test_accuracy: 0.38184\n",
            "[  315/3000] train_loss: 2.74422, train_accuracy: 0.29000 ; test_loss: 2.31707, test_accuracy: 0.38564\n",
            "[  320/3000] train_loss: 2.15680, train_accuracy: 0.43000 ; test_loss: 2.29425, test_accuracy: 0.38760\n",
            "[  325/3000] train_loss: 2.40504, train_accuracy: 0.38000 ; test_loss: 2.29222, test_accuracy: 0.38496\n",
            "[  330/3000] train_loss: 2.17184, train_accuracy: 0.36500 ; test_loss: 2.26085, test_accuracy: 0.38520\n",
            "[  335/3000] train_loss: 2.31837, train_accuracy: 0.39000 ; test_loss: 2.25706, test_accuracy: 0.38768\n",
            "[  340/3000] train_loss: 2.19363, train_accuracy: 0.42000 ; test_loss: 2.23277, test_accuracy: 0.39064\n",
            "[  345/3000] train_loss: 2.27876, train_accuracy: 0.36500 ; test_loss: 2.22502, test_accuracy: 0.39224\n",
            "[  350/3000] train_loss: 2.27327, train_accuracy: 0.38000 ; test_loss: 2.18857, test_accuracy: 0.39232\n",
            "[  355/3000] train_loss: 1.88710, train_accuracy: 0.45000 ; test_loss: 2.18683, test_accuracy: 0.39016\n",
            "[  360/3000] train_loss: 2.08807, train_accuracy: 0.44500 ; test_loss: 2.16384, test_accuracy: 0.39380\n",
            "[  365/3000] train_loss: 2.03384, train_accuracy: 0.40500 ; test_loss: 2.15310, test_accuracy: 0.39540\n",
            "[  370/3000] train_loss: 2.20986, train_accuracy: 0.39000 ; test_loss: 2.15084, test_accuracy: 0.39848\n",
            "[  375/3000] train_loss: 1.92918, train_accuracy: 0.39500 ; test_loss: 2.12192, test_accuracy: 0.40060\n",
            "[  380/3000] train_loss: 1.86382, train_accuracy: 0.46000 ; test_loss: 2.10796, test_accuracy: 0.39928\n",
            "[  385/3000] train_loss: 2.23849, train_accuracy: 0.34000 ; test_loss: 2.09009, test_accuracy: 0.40424\n",
            "[  390/3000] train_loss: 1.90450, train_accuracy: 0.45500 ; test_loss: 2.08222, test_accuracy: 0.40604\n",
            "[  395/3000] train_loss: 2.15898, train_accuracy: 0.42500 ; test_loss: 2.07029, test_accuracy: 0.40296\n",
            "[  400/3000] train_loss: 1.98064, train_accuracy: 0.41000 ; test_loss: 2.05223, test_accuracy: 0.40536\n",
            "[  405/3000] train_loss: 2.11193, train_accuracy: 0.44500 ; test_loss: 2.03941, test_accuracy: 0.41088\n",
            "[  410/3000] train_loss: 1.92642, train_accuracy: 0.43000 ; test_loss: 2.02479, test_accuracy: 0.41176\n",
            "[  415/3000] train_loss: 2.08859, train_accuracy: 0.38500 ; test_loss: 2.01631, test_accuracy: 0.41196\n",
            "[  420/3000] train_loss: 2.09249, train_accuracy: 0.37500 ; test_loss: 2.01245, test_accuracy: 0.40688\n",
            "[  425/3000] train_loss: 1.96446, train_accuracy: 0.39000 ; test_loss: 1.98820, test_accuracy: 0.40916\n",
            "[  430/3000] train_loss: 2.02186, train_accuracy: 0.42500 ; test_loss: 1.97524, test_accuracy: 0.40876\n",
            "[  435/3000] train_loss: 2.17223, train_accuracy: 0.38500 ; test_loss: 1.96509, test_accuracy: 0.41540\n",
            "[  440/3000] train_loss: 2.10636, train_accuracy: 0.39000 ; test_loss: 1.96046, test_accuracy: 0.41364\n",
            "[  445/3000] train_loss: 2.01031, train_accuracy: 0.39000 ; test_loss: 1.95280, test_accuracy: 0.41444\n",
            "[  450/3000] train_loss: 1.93894, train_accuracy: 0.41000 ; test_loss: 1.93888, test_accuracy: 0.41672\n",
            "[  455/3000] train_loss: 1.99029, train_accuracy: 0.38000 ; test_loss: 1.93016, test_accuracy: 0.41612\n",
            "[  460/3000] train_loss: 1.66653, train_accuracy: 0.50500 ; test_loss: 1.92045, test_accuracy: 0.41964\n",
            "[  465/3000] train_loss: 1.91862, train_accuracy: 0.39000 ; test_loss: 1.90885, test_accuracy: 0.42132\n",
            "[  470/3000] train_loss: 2.05520, train_accuracy: 0.38000 ; test_loss: 1.89514, test_accuracy: 0.41848\n",
            "[  475/3000] train_loss: 1.93609, train_accuracy: 0.38000 ; test_loss: 1.88437, test_accuracy: 0.41944\n",
            "[  480/3000] train_loss: 1.69798, train_accuracy: 0.45500 ; test_loss: 1.87758, test_accuracy: 0.42240\n",
            "[  485/3000] train_loss: 1.95991, train_accuracy: 0.40500 ; test_loss: 1.86986, test_accuracy: 0.42540\n",
            "[  490/3000] train_loss: 1.84888, train_accuracy: 0.40500 ; test_loss: 1.86732, test_accuracy: 0.42892\n",
            "[  495/3000] train_loss: 1.88786, train_accuracy: 0.41500 ; test_loss: 1.85779, test_accuracy: 0.42804\n",
            "[  500/3000] train_loss: 1.65209, train_accuracy: 0.46500 ; test_loss: 1.85487, test_accuracy: 0.42584\n",
            "[  505/3000] train_loss: 2.04094, train_accuracy: 0.37500 ; test_loss: 1.85256, test_accuracy: 0.42452\n",
            "[  510/3000] train_loss: 1.86357, train_accuracy: 0.39500 ; test_loss: 1.83581, test_accuracy: 0.42752\n",
            "[  515/3000] train_loss: 1.76555, train_accuracy: 0.44000 ; test_loss: 1.85033, test_accuracy: 0.42192\n",
            "[  520/3000] train_loss: 1.70596, train_accuracy: 0.42000 ; test_loss: 1.82915, test_accuracy: 0.42932\n",
            "[  525/3000] train_loss: 1.87618, train_accuracy: 0.38000 ; test_loss: 1.82603, test_accuracy: 0.43224\n",
            "[  530/3000] train_loss: 1.65976, train_accuracy: 0.46500 ; test_loss: 1.81849, test_accuracy: 0.43096\n",
            "[  535/3000] train_loss: 1.53165, train_accuracy: 0.46000 ; test_loss: 1.81476, test_accuracy: 0.43148\n",
            "[  540/3000] train_loss: 2.08921, train_accuracy: 0.38000 ; test_loss: 1.81021, test_accuracy: 0.43340\n",
            "[  545/3000] train_loss: 1.73522, train_accuracy: 0.43000 ; test_loss: 1.80048, test_accuracy: 0.43172\n",
            "[  550/3000] train_loss: 1.61307, train_accuracy: 0.47000 ; test_loss: 1.79294, test_accuracy: 0.43184\n",
            "[  555/3000] train_loss: 1.94424, train_accuracy: 0.38500 ; test_loss: 1.78523, test_accuracy: 0.43572\n",
            "[  560/3000] train_loss: 1.76310, train_accuracy: 0.44000 ; test_loss: 1.77781, test_accuracy: 0.43796\n",
            "[  565/3000] train_loss: 1.55281, train_accuracy: 0.52500 ; test_loss: 1.77340, test_accuracy: 0.43560\n",
            "[  570/3000] train_loss: 1.76776, train_accuracy: 0.43500 ; test_loss: 1.77046, test_accuracy: 0.43728\n",
            "[  575/3000] train_loss: 1.79161, train_accuracy: 0.43000 ; test_loss: 1.76033, test_accuracy: 0.44112\n",
            "[  580/3000] train_loss: 1.81663, train_accuracy: 0.39000 ; test_loss: 1.75729, test_accuracy: 0.43876\n",
            "[  585/3000] train_loss: 1.73775, train_accuracy: 0.41000 ; test_loss: 1.75889, test_accuracy: 0.43812\n",
            "[  590/3000] train_loss: 1.67866, train_accuracy: 0.46000 ; test_loss: 1.74827, test_accuracy: 0.43908\n",
            "[  595/3000] train_loss: 1.83461, train_accuracy: 0.41500 ; test_loss: 1.74714, test_accuracy: 0.44204\n",
            "[  600/3000] train_loss: 1.63754, train_accuracy: 0.46500 ; test_loss: 1.74837, test_accuracy: 0.44008\n",
            "[  605/3000] train_loss: 1.63843, train_accuracy: 0.49500 ; test_loss: 1.73642, test_accuracy: 0.44212\n",
            "[  610/3000] train_loss: 1.81014, train_accuracy: 0.40000 ; test_loss: 1.73632, test_accuracy: 0.44660\n",
            "[  615/3000] train_loss: 1.80750, train_accuracy: 0.45500 ; test_loss: 1.73706, test_accuracy: 0.44656\n",
            "[  620/3000] train_loss: 1.57482, train_accuracy: 0.50000 ; test_loss: 1.72577, test_accuracy: 0.44384\n",
            "[  625/3000] train_loss: 1.59963, train_accuracy: 0.46500 ; test_loss: 1.72021, test_accuracy: 0.44724\n",
            "[  630/3000] train_loss: 1.61209, train_accuracy: 0.48000 ; test_loss: 1.71480, test_accuracy: 0.44988\n",
            "[  635/3000] train_loss: 1.71721, train_accuracy: 0.49000 ; test_loss: 1.71430, test_accuracy: 0.45248\n",
            "[  640/3000] train_loss: 1.82666, train_accuracy: 0.42000 ; test_loss: 1.71559, test_accuracy: 0.45092\n",
            "[  645/3000] train_loss: 1.71455, train_accuracy: 0.49000 ; test_loss: 1.69772, test_accuracy: 0.45692\n",
            "[  650/3000] train_loss: 1.69170, train_accuracy: 0.48000 ; test_loss: 1.70062, test_accuracy: 0.45788\n",
            "[  655/3000] train_loss: 1.60849, train_accuracy: 0.44000 ; test_loss: 1.69824, test_accuracy: 0.45296\n",
            "[  660/3000] train_loss: 1.75167, train_accuracy: 0.43500 ; test_loss: 1.70143, test_accuracy: 0.45132\n",
            "[  665/3000] train_loss: 1.63668, train_accuracy: 0.47500 ; test_loss: 1.68721, test_accuracy: 0.45396\n",
            "[  670/3000] train_loss: 1.48392, train_accuracy: 0.45000 ; test_loss: 1.68490, test_accuracy: 0.45508\n",
            "[  675/3000] train_loss: 1.52976, train_accuracy: 0.46000 ; test_loss: 1.68702, test_accuracy: 0.45612\n",
            "[  680/3000] train_loss: 1.50265, train_accuracy: 0.53000 ; test_loss: 1.67662, test_accuracy: 0.46124\n",
            "[  685/3000] train_loss: 1.68955, train_accuracy: 0.40500 ; test_loss: 1.68726, test_accuracy: 0.45540\n",
            "[  690/3000] train_loss: 1.78283, train_accuracy: 0.41500 ; test_loss: 1.67836, test_accuracy: 0.45940\n",
            "[  695/3000] train_loss: 1.62909, train_accuracy: 0.46000 ; test_loss: 1.67826, test_accuracy: 0.45972\n",
            "[  700/3000] train_loss: 1.56076, train_accuracy: 0.47500 ; test_loss: 1.67120, test_accuracy: 0.46208\n",
            "[  705/3000] train_loss: 1.64133, train_accuracy: 0.43000 ; test_loss: 1.67033, test_accuracy: 0.46156\n",
            "[  710/3000] train_loss: 1.53057, train_accuracy: 0.48000 ; test_loss: 1.66204, test_accuracy: 0.46312\n",
            "[  715/3000] train_loss: 1.59480, train_accuracy: 0.51500 ; test_loss: 1.65427, test_accuracy: 0.46492\n",
            "[  720/3000] train_loss: 1.78815, train_accuracy: 0.43500 ; test_loss: 1.65773, test_accuracy: 0.46324\n",
            "[  725/3000] train_loss: 1.62621, train_accuracy: 0.45500 ; test_loss: 1.65157, test_accuracy: 0.46600\n",
            "[  730/3000] train_loss: 1.57651, train_accuracy: 0.43000 ; test_loss: 1.65056, test_accuracy: 0.46788\n",
            "[  735/3000] train_loss: 1.65434, train_accuracy: 0.44500 ; test_loss: 1.65161, test_accuracy: 0.46836\n",
            "[  740/3000] train_loss: 1.49233, train_accuracy: 0.49000 ; test_loss: 1.64525, test_accuracy: 0.46680\n",
            "[  745/3000] train_loss: 1.49714, train_accuracy: 0.53500 ; test_loss: 1.64578, test_accuracy: 0.46408\n",
            "[  750/3000] train_loss: 1.72417, train_accuracy: 0.40000 ; test_loss: 1.64301, test_accuracy: 0.46868\n",
            "[  755/3000] train_loss: 1.52376, train_accuracy: 0.48500 ; test_loss: 1.63552, test_accuracy: 0.47128\n",
            "[  760/3000] train_loss: 1.58630, train_accuracy: 0.44500 ; test_loss: 1.63548, test_accuracy: 0.46956\n",
            "[  765/3000] train_loss: 1.59438, train_accuracy: 0.51000 ; test_loss: 1.62987, test_accuracy: 0.47036\n",
            "[  770/3000] train_loss: 1.40893, train_accuracy: 0.49500 ; test_loss: 1.62725, test_accuracy: 0.47080\n",
            "[  775/3000] train_loss: 1.59219, train_accuracy: 0.49000 ; test_loss: 1.62188, test_accuracy: 0.47440\n",
            "[  780/3000] train_loss: 1.57101, train_accuracy: 0.52000 ; test_loss: 1.62132, test_accuracy: 0.47132\n",
            "[  785/3000] train_loss: 1.61420, train_accuracy: 0.49500 ; test_loss: 1.62361, test_accuracy: 0.46904\n",
            "[  790/3000] train_loss: 1.54306, train_accuracy: 0.43500 ; test_loss: 1.61879, test_accuracy: 0.46704\n",
            "[  795/3000] train_loss: 1.46273, train_accuracy: 0.52500 ; test_loss: 1.61345, test_accuracy: 0.47268\n",
            "[  800/3000] train_loss: 1.71251, train_accuracy: 0.38500 ; test_loss: 1.61761, test_accuracy: 0.47664\n",
            "[  805/3000] train_loss: 1.67232, train_accuracy: 0.45500 ; test_loss: 1.60002, test_accuracy: 0.47736\n",
            "[  810/3000] train_loss: 1.53015, train_accuracy: 0.50500 ; test_loss: 1.60632, test_accuracy: 0.47528\n",
            "[  815/3000] train_loss: 1.64935, train_accuracy: 0.50500 ; test_loss: 1.59749, test_accuracy: 0.48052\n",
            "[  820/3000] train_loss: 1.48887, train_accuracy: 0.50000 ; test_loss: 1.59694, test_accuracy: 0.48232\n",
            "[  825/3000] train_loss: 1.45844, train_accuracy: 0.53500 ; test_loss: 1.58753, test_accuracy: 0.48372\n",
            "[  830/3000] train_loss: 1.70455, train_accuracy: 0.47000 ; test_loss: 1.58386, test_accuracy: 0.48660\n",
            "[  835/3000] train_loss: 1.62288, train_accuracy: 0.42500 ; test_loss: 1.59567, test_accuracy: 0.48264\n",
            "[  840/3000] train_loss: 1.72152, train_accuracy: 0.49500 ; test_loss: 1.58703, test_accuracy: 0.48136\n",
            "[  845/3000] train_loss: 1.65909, train_accuracy: 0.44500 ; test_loss: 1.58275, test_accuracy: 0.48248\n",
            "[  850/3000] train_loss: 1.55282, train_accuracy: 0.50500 ; test_loss: 1.59093, test_accuracy: 0.47840\n",
            "[  855/3000] train_loss: 1.65848, train_accuracy: 0.44500 ; test_loss: 1.57407, test_accuracy: 0.48224\n",
            "[  860/3000] train_loss: 1.50860, train_accuracy: 0.50000 ; test_loss: 1.57366, test_accuracy: 0.48400\n",
            "[  865/3000] train_loss: 1.60416, train_accuracy: 0.44500 ; test_loss: 1.57069, test_accuracy: 0.48252\n",
            "[  870/3000] train_loss: 1.66894, train_accuracy: 0.43000 ; test_loss: 1.56918, test_accuracy: 0.48100\n",
            "[  875/3000] train_loss: 1.67556, train_accuracy: 0.46500 ; test_loss: 1.57013, test_accuracy: 0.47924\n",
            "[  880/3000] train_loss: 1.67676, train_accuracy: 0.46500 ; test_loss: 1.57220, test_accuracy: 0.48216\n",
            "[  885/3000] train_loss: 1.28119, train_accuracy: 0.55000 ; test_loss: 1.56910, test_accuracy: 0.48284\n",
            "[  890/3000] train_loss: 1.42924, train_accuracy: 0.49500 ; test_loss: 1.56097, test_accuracy: 0.48676\n",
            "[  895/3000] train_loss: 1.33320, train_accuracy: 0.47500 ; test_loss: 1.55189, test_accuracy: 0.48648\n",
            "[  900/3000] train_loss: 1.47571, train_accuracy: 0.45500 ; test_loss: 1.54979, test_accuracy: 0.48536\n",
            "[  905/3000] train_loss: 1.61477, train_accuracy: 0.44500 ; test_loss: 1.55097, test_accuracy: 0.48836\n",
            "[  910/3000] train_loss: 1.59915, train_accuracy: 0.48500 ; test_loss: 1.55304, test_accuracy: 0.49008\n",
            "[  915/3000] train_loss: 1.36018, train_accuracy: 0.54000 ; test_loss: 1.54729, test_accuracy: 0.48952\n",
            "[  920/3000] train_loss: 1.54469, train_accuracy: 0.47000 ; test_loss: 1.54907, test_accuracy: 0.48784\n",
            "[  925/3000] train_loss: 1.35876, train_accuracy: 0.51500 ; test_loss: 1.54722, test_accuracy: 0.48588\n",
            "[  930/3000] train_loss: 1.71526, train_accuracy: 0.49000 ; test_loss: 1.54624, test_accuracy: 0.48996\n",
            "[  935/3000] train_loss: 1.61650, train_accuracy: 0.46000 ; test_loss: 1.54442, test_accuracy: 0.49164\n",
            "[  940/3000] train_loss: 1.44924, train_accuracy: 0.56500 ; test_loss: 1.53682, test_accuracy: 0.49304\n",
            "[  945/3000] train_loss: 1.57025, train_accuracy: 0.53000 ; test_loss: 1.53770, test_accuracy: 0.49276\n",
            "[  950/3000] train_loss: 1.38596, train_accuracy: 0.53500 ; test_loss: 1.54175, test_accuracy: 0.49268\n",
            "[  955/3000] train_loss: 1.66548, train_accuracy: 0.41000 ; test_loss: 1.53508, test_accuracy: 0.49344\n",
            "[  960/3000] train_loss: 1.59572, train_accuracy: 0.49000 ; test_loss: 1.53526, test_accuracy: 0.49180\n",
            "[  965/3000] train_loss: 1.46550, train_accuracy: 0.54500 ; test_loss: 1.53339, test_accuracy: 0.49840\n",
            "[  970/3000] train_loss: 1.29149, train_accuracy: 0.55500 ; test_loss: 1.53112, test_accuracy: 0.49792\n",
            "[  975/3000] train_loss: 1.43624, train_accuracy: 0.55500 ; test_loss: 1.53149, test_accuracy: 0.49248\n",
            "[  980/3000] train_loss: 1.41399, train_accuracy: 0.55500 ; test_loss: 1.52800, test_accuracy: 0.49416\n",
            "[  985/3000] train_loss: 1.50851, train_accuracy: 0.52500 ; test_loss: 1.51993, test_accuracy: 0.50064\n",
            "[  990/3000] train_loss: 1.51001, train_accuracy: 0.47500 ; test_loss: 1.52163, test_accuracy: 0.49964\n",
            "[  995/3000] train_loss: 1.37516, train_accuracy: 0.55500 ; test_loss: 1.51218, test_accuracy: 0.49964\n",
            "[ 1000/3000] train_loss: 1.54574, train_accuracy: 0.50500 ; test_loss: 1.52011, test_accuracy: 0.49692\n",
            "[ 1005/3000] train_loss: 1.43601, train_accuracy: 0.54500 ; test_loss: 1.50812, test_accuracy: 0.50260\n",
            "[ 1010/3000] train_loss: 1.43312, train_accuracy: 0.51500 ; test_loss: 1.51041, test_accuracy: 0.50032\n",
            "[ 1015/3000] train_loss: 1.60552, train_accuracy: 0.49500 ; test_loss: 1.50553, test_accuracy: 0.50120\n",
            "[ 1020/3000] train_loss: 1.35557, train_accuracy: 0.56000 ; test_loss: 1.50789, test_accuracy: 0.50044\n",
            "[ 1025/3000] train_loss: 1.33326, train_accuracy: 0.54000 ; test_loss: 1.49486, test_accuracy: 0.50632\n",
            "[ 1030/3000] train_loss: 1.60862, train_accuracy: 0.50500 ; test_loss: 1.50264, test_accuracy: 0.50648\n",
            "[ 1035/3000] train_loss: 1.51575, train_accuracy: 0.57000 ; test_loss: 1.50866, test_accuracy: 0.50648\n",
            "[ 1040/3000] train_loss: 1.35270, train_accuracy: 0.54000 ; test_loss: 1.49684, test_accuracy: 0.50280\n",
            "[ 1045/3000] train_loss: 1.44724, train_accuracy: 0.51500 ; test_loss: 1.49021, test_accuracy: 0.50568\n",
            "[ 1050/3000] train_loss: 1.30692, train_accuracy: 0.56500 ; test_loss: 1.49324, test_accuracy: 0.50824\n",
            "[ 1055/3000] train_loss: 1.54274, train_accuracy: 0.49500 ; test_loss: 1.48487, test_accuracy: 0.50792\n",
            "[ 1060/3000] train_loss: 1.51293, train_accuracy: 0.46500 ; test_loss: 1.48549, test_accuracy: 0.50624\n",
            "[ 1065/3000] train_loss: 1.55615, train_accuracy: 0.46000 ; test_loss: 1.47926, test_accuracy: 0.50568\n",
            "[ 1070/3000] train_loss: 1.36480, train_accuracy: 0.56000 ; test_loss: 1.47625, test_accuracy: 0.50704\n",
            "[ 1075/3000] train_loss: 1.36186, train_accuracy: 0.53000 ; test_loss: 1.47587, test_accuracy: 0.50776\n",
            "[ 1080/3000] train_loss: 1.29252, train_accuracy: 0.54500 ; test_loss: 1.48399, test_accuracy: 0.50816\n",
            "[ 1085/3000] train_loss: 1.36203, train_accuracy: 0.55500 ; test_loss: 1.47245, test_accuracy: 0.50944\n",
            "[ 1090/3000] train_loss: 1.28893, train_accuracy: 0.54500 ; test_loss: 1.47492, test_accuracy: 0.50768\n",
            "[ 1095/3000] train_loss: 1.53213, train_accuracy: 0.46500 ; test_loss: 1.47183, test_accuracy: 0.50816\n",
            "[ 1100/3000] train_loss: 1.42144, train_accuracy: 0.55500 ; test_loss: 1.47053, test_accuracy: 0.51024\n",
            "[ 1105/3000] train_loss: 1.50993, train_accuracy: 0.47500 ; test_loss: 1.47599, test_accuracy: 0.51132\n",
            "[ 1110/3000] train_loss: 1.31297, train_accuracy: 0.53000 ; test_loss: 1.47892, test_accuracy: 0.51152\n",
            "[ 1115/3000] train_loss: 1.31639, train_accuracy: 0.53500 ; test_loss: 1.47818, test_accuracy: 0.51100\n",
            "[ 1120/3000] train_loss: 1.38254, train_accuracy: 0.54500 ; test_loss: 1.47605, test_accuracy: 0.50908\n",
            "[ 1125/3000] train_loss: 1.37042, train_accuracy: 0.51500 ; test_loss: 1.46664, test_accuracy: 0.51268\n",
            "[ 1130/3000] train_loss: 1.56808, train_accuracy: 0.48500 ; test_loss: 1.45578, test_accuracy: 0.51448\n",
            "[ 1135/3000] train_loss: 1.37285, train_accuracy: 0.49000 ; test_loss: 1.46076, test_accuracy: 0.51292\n",
            "[ 1140/3000] train_loss: 1.31813, train_accuracy: 0.57000 ; test_loss: 1.45651, test_accuracy: 0.51308\n",
            "[ 1145/3000] train_loss: 1.29695, train_accuracy: 0.55500 ; test_loss: 1.45450, test_accuracy: 0.51372\n",
            "[ 1150/3000] train_loss: 1.56116, train_accuracy: 0.54500 ; test_loss: 1.46166, test_accuracy: 0.51324\n",
            "[ 1155/3000] train_loss: 1.27993, train_accuracy: 0.57500 ; test_loss: 1.45147, test_accuracy: 0.51752\n",
            "[ 1160/3000] train_loss: 1.34730, train_accuracy: 0.58000 ; test_loss: 1.45710, test_accuracy: 0.51424\n",
            "[ 1165/3000] train_loss: 1.36570, train_accuracy: 0.48500 ; test_loss: 1.44996, test_accuracy: 0.51816\n",
            "[ 1170/3000] train_loss: 1.36650, train_accuracy: 0.53500 ; test_loss: 1.44666, test_accuracy: 0.51692\n",
            "[ 1175/3000] train_loss: 1.43795, train_accuracy: 0.55500 ; test_loss: 1.44525, test_accuracy: 0.51836\n",
            "[ 1180/3000] train_loss: 1.51034, train_accuracy: 0.43500 ; test_loss: 1.45003, test_accuracy: 0.51816\n",
            "[ 1185/3000] train_loss: 1.47481, train_accuracy: 0.56000 ; test_loss: 1.44787, test_accuracy: 0.51800\n",
            "[ 1190/3000] train_loss: 1.39496, train_accuracy: 0.51000 ; test_loss: 1.44803, test_accuracy: 0.51788\n",
            "[ 1195/3000] train_loss: 1.52159, train_accuracy: 0.48500 ; test_loss: 1.44214, test_accuracy: 0.51820\n",
            "[ 1200/3000] train_loss: 1.39695, train_accuracy: 0.53000 ; test_loss: 1.44185, test_accuracy: 0.51892\n",
            "[ 1205/3000] train_loss: 1.30584, train_accuracy: 0.56500 ; test_loss: 1.45510, test_accuracy: 0.51976\n",
            "[ 1210/3000] train_loss: 1.17491, train_accuracy: 0.61000 ; test_loss: 1.43785, test_accuracy: 0.52192\n",
            "[ 1215/3000] train_loss: 1.27810, train_accuracy: 0.54500 ; test_loss: 1.44586, test_accuracy: 0.51864\n",
            "[ 1220/3000] train_loss: 1.36554, train_accuracy: 0.57500 ; test_loss: 1.43491, test_accuracy: 0.52368\n",
            "[ 1225/3000] train_loss: 1.34312, train_accuracy: 0.56000 ; test_loss: 1.44395, test_accuracy: 0.52436\n",
            "[ 1230/3000] train_loss: 1.33725, train_accuracy: 0.60000 ; test_loss: 1.43177, test_accuracy: 0.52872\n",
            "[ 1235/3000] train_loss: 1.40923, train_accuracy: 0.49500 ; test_loss: 1.44927, test_accuracy: 0.51908\n",
            "[ 1240/3000] train_loss: 1.38470, train_accuracy: 0.53000 ; test_loss: 1.43583, test_accuracy: 0.52136\n",
            "[ 1245/3000] train_loss: 1.35066, train_accuracy: 0.54500 ; test_loss: 1.43027, test_accuracy: 0.52312\n",
            "[ 1250/3000] train_loss: 1.46108, train_accuracy: 0.56500 ; test_loss: 1.42554, test_accuracy: 0.52856\n",
            "[ 1255/3000] train_loss: 1.31080, train_accuracy: 0.53500 ; test_loss: 1.42363, test_accuracy: 0.53076\n",
            "[ 1260/3000] train_loss: 1.23068, train_accuracy: 0.58500 ; test_loss: 1.42537, test_accuracy: 0.53000\n",
            "[ 1265/3000] train_loss: 1.35205, train_accuracy: 0.52500 ; test_loss: 1.41847, test_accuracy: 0.52812\n",
            "[ 1270/3000] train_loss: 1.42665, train_accuracy: 0.50500 ; test_loss: 1.41942, test_accuracy: 0.52844\n",
            "[ 1275/3000] train_loss: 1.40472, train_accuracy: 0.55500 ; test_loss: 1.41192, test_accuracy: 0.52936\n",
            "[ 1280/3000] train_loss: 1.29479, train_accuracy: 0.58000 ; test_loss: 1.41635, test_accuracy: 0.52928\n",
            "[ 1285/3000] train_loss: 1.46645, train_accuracy: 0.48500 ; test_loss: 1.41007, test_accuracy: 0.53240\n",
            "[ 1290/3000] train_loss: 1.32939, train_accuracy: 0.57500 ; test_loss: 1.41046, test_accuracy: 0.53128\n",
            "[ 1295/3000] train_loss: 1.29704, train_accuracy: 0.56000 ; test_loss: 1.41600, test_accuracy: 0.52476\n",
            "[ 1300/3000] train_loss: 1.45515, train_accuracy: 0.51000 ; test_loss: 1.41320, test_accuracy: 0.52996\n",
            "[ 1305/3000] train_loss: 1.43425, train_accuracy: 0.54000 ; test_loss: 1.40955, test_accuracy: 0.52976\n",
            "[ 1310/3000] train_loss: 1.42815, train_accuracy: 0.56000 ; test_loss: 1.40981, test_accuracy: 0.53036\n",
            "[ 1315/3000] train_loss: 1.36509, train_accuracy: 0.52500 ; test_loss: 1.41749, test_accuracy: 0.53032\n",
            "[ 1320/3000] train_loss: 1.31845, train_accuracy: 0.56500 ; test_loss: 1.40494, test_accuracy: 0.53416\n",
            "[ 1325/3000] train_loss: 1.33689, train_accuracy: 0.60500 ; test_loss: 1.40716, test_accuracy: 0.53308\n",
            "[ 1330/3000] train_loss: 1.33282, train_accuracy: 0.58000 ; test_loss: 1.40056, test_accuracy: 0.53612\n",
            "[ 1335/3000] train_loss: 1.32918, train_accuracy: 0.53000 ; test_loss: 1.40764, test_accuracy: 0.53176\n",
            "[ 1340/3000] train_loss: 1.35552, train_accuracy: 0.55000 ; test_loss: 1.40677, test_accuracy: 0.53436\n",
            "[ 1345/3000] train_loss: 1.17165, train_accuracy: 0.60500 ; test_loss: 1.40093, test_accuracy: 0.53360\n",
            "[ 1350/3000] train_loss: 1.52859, train_accuracy: 0.52000 ; test_loss: 1.40306, test_accuracy: 0.53276\n",
            "[ 1355/3000] train_loss: 1.56057, train_accuracy: 0.49000 ; test_loss: 1.39971, test_accuracy: 0.52988\n",
            "[ 1360/3000] train_loss: 1.61442, train_accuracy: 0.53000 ; test_loss: 1.39489, test_accuracy: 0.53524\n",
            "[ 1365/3000] train_loss: 1.36559, train_accuracy: 0.58000 ; test_loss: 1.39666, test_accuracy: 0.53440\n",
            "[ 1370/3000] train_loss: 1.31746, train_accuracy: 0.58000 ; test_loss: 1.39038, test_accuracy: 0.53752\n",
            "[ 1375/3000] train_loss: 1.30287, train_accuracy: 0.57500 ; test_loss: 1.40788, test_accuracy: 0.53256\n",
            "[ 1380/3000] train_loss: 1.30225, train_accuracy: 0.55000 ; test_loss: 1.39362, test_accuracy: 0.53532\n",
            "[ 1385/3000] train_loss: 1.34239, train_accuracy: 0.51000 ; test_loss: 1.39578, test_accuracy: 0.53076\n",
            "[ 1390/3000] train_loss: 1.49386, train_accuracy: 0.52000 ; test_loss: 1.38864, test_accuracy: 0.53832\n",
            "[ 1395/3000] train_loss: 1.36562, train_accuracy: 0.55500 ; test_loss: 1.39701, test_accuracy: 0.53388\n",
            "[ 1400/3000] train_loss: 1.24858, train_accuracy: 0.61000 ; test_loss: 1.39201, test_accuracy: 0.53704\n",
            "[ 1405/3000] train_loss: 1.28292, train_accuracy: 0.60000 ; test_loss: 1.39214, test_accuracy: 0.53652\n",
            "[ 1410/3000] train_loss: 1.29726, train_accuracy: 0.50500 ; test_loss: 1.39030, test_accuracy: 0.53728\n",
            "[ 1415/3000] train_loss: 1.29106, train_accuracy: 0.58000 ; test_loss: 1.38463, test_accuracy: 0.53860\n",
            "[ 1420/3000] train_loss: 1.28117, train_accuracy: 0.51500 ; test_loss: 1.38715, test_accuracy: 0.53752\n",
            "[ 1425/3000] train_loss: 1.63358, train_accuracy: 0.48500 ; test_loss: 1.37880, test_accuracy: 0.54024\n",
            "[ 1430/3000] train_loss: 1.36234, train_accuracy: 0.52500 ; test_loss: 1.38428, test_accuracy: 0.54200\n",
            "[ 1435/3000] train_loss: 1.36060, train_accuracy: 0.57000 ; test_loss: 1.37727, test_accuracy: 0.54336\n",
            "[ 1440/3000] train_loss: 1.33849, train_accuracy: 0.56500 ; test_loss: 1.37407, test_accuracy: 0.54424\n",
            "[ 1445/3000] train_loss: 1.27237, train_accuracy: 0.54000 ; test_loss: 1.37464, test_accuracy: 0.54192\n",
            "[ 1450/3000] train_loss: 1.22174, train_accuracy: 0.57000 ; test_loss: 1.38263, test_accuracy: 0.53624\n",
            "[ 1455/3000] train_loss: 1.31165, train_accuracy: 0.58500 ; test_loss: 1.38660, test_accuracy: 0.53720\n",
            "[ 1460/3000] train_loss: 1.36804, train_accuracy: 0.54000 ; test_loss: 1.37293, test_accuracy: 0.54412\n",
            "[ 1465/3000] train_loss: 1.36685, train_accuracy: 0.56000 ; test_loss: 1.37317, test_accuracy: 0.54392\n",
            "[ 1470/3000] train_loss: 1.41446, train_accuracy: 0.53000 ; test_loss: 1.37420, test_accuracy: 0.54368\n",
            "[ 1475/3000] train_loss: 1.28897, train_accuracy: 0.58500 ; test_loss: 1.37333, test_accuracy: 0.54308\n",
            "[ 1480/3000] train_loss: 1.38306, train_accuracy: 0.53000 ; test_loss: 1.37064, test_accuracy: 0.54412\n",
            "[ 1485/3000] train_loss: 1.45493, train_accuracy: 0.55000 ; test_loss: 1.36832, test_accuracy: 0.54336\n",
            "[ 1490/3000] train_loss: 1.31707, train_accuracy: 0.59500 ; test_loss: 1.37149, test_accuracy: 0.53940\n",
            "[ 1495/3000] train_loss: 1.33346, train_accuracy: 0.56500 ; test_loss: 1.37526, test_accuracy: 0.54084\n",
            "[ 1500/3000] train_loss: 1.32739, train_accuracy: 0.52500 ; test_loss: 1.36717, test_accuracy: 0.54248\n",
            "[ 1505/3000] train_loss: 1.42587, train_accuracy: 0.50000 ; test_loss: 1.37297, test_accuracy: 0.53972\n",
            "[ 1510/3000] train_loss: 1.39012, train_accuracy: 0.54500 ; test_loss: 1.36300, test_accuracy: 0.54640\n",
            "[ 1515/3000] train_loss: 1.39183, train_accuracy: 0.56500 ; test_loss: 1.36209, test_accuracy: 0.54440\n",
            "[ 1520/3000] train_loss: 1.40355, train_accuracy: 0.49500 ; test_loss: 1.35836, test_accuracy: 0.54788\n",
            "[ 1525/3000] train_loss: 1.32947, train_accuracy: 0.56500 ; test_loss: 1.35637, test_accuracy: 0.54664\n",
            "[ 1530/3000] train_loss: 1.31557, train_accuracy: 0.56500 ; test_loss: 1.36150, test_accuracy: 0.54572\n",
            "[ 1535/3000] train_loss: 1.32548, train_accuracy: 0.56500 ; test_loss: 1.35180, test_accuracy: 0.54936\n",
            "[ 1540/3000] train_loss: 1.20099, train_accuracy: 0.57000 ; test_loss: 1.35060, test_accuracy: 0.54732\n",
            "[ 1545/3000] train_loss: 1.44993, train_accuracy: 0.47000 ; test_loss: 1.35864, test_accuracy: 0.54736\n",
            "[ 1550/3000] train_loss: 1.17675, train_accuracy: 0.63500 ; test_loss: 1.35084, test_accuracy: 0.55220\n",
            "[ 1555/3000] train_loss: 1.33968, train_accuracy: 0.56500 ; test_loss: 1.34516, test_accuracy: 0.55040\n",
            "[ 1560/3000] train_loss: 1.35962, train_accuracy: 0.59000 ; test_loss: 1.35452, test_accuracy: 0.54540\n",
            "[ 1565/3000] train_loss: 1.35451, train_accuracy: 0.52500 ; test_loss: 1.35193, test_accuracy: 0.54764\n",
            "[ 1570/3000] train_loss: 1.43848, train_accuracy: 0.54500 ; test_loss: 1.34838, test_accuracy: 0.55100\n",
            "[ 1575/3000] train_loss: 1.31735, train_accuracy: 0.49500 ; test_loss: 1.35692, test_accuracy: 0.54540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "07B5sNMRlMLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rd\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "# Preparing the data set\n",
        "with open('AI_quick_draw.pickle', 'rb') as open_ai_quick:\n",
        "    data_train = pickle.load(open_ai_quick)\n",
        "    label_train1 = pickle.load(open_ai_quick)\n",
        "    data_test = pickle.load(open_ai_quick)\n",
        "    label_test1 = pickle.load(open_ai_quick)\n",
        "n_classes = len(np.unique(label_train1))\n",
        "# convert labels to 0-1 hot encoding\n",
        "label_train = np.zeros((label_train1.shape[0], n_classes))\n",
        "a = np.arange(label_train1.shape[0], dtype=np.int64)\n",
        "b = np.array(label_train1, dtype=np.int64).reshape((label_train1.shape[0],))\n",
        "label_train[a, b] = 1\n",
        "\n",
        "label_test = np.zeros((label_test1.shape[0], n_classes))\n",
        "c = np.arange(label_test1.shape[0], dtype=np.int64)\n",
        "d = np.array(label_test1, dtype=np.int64).reshape((label_test1.shape[0],))\n",
        "label_test[c, d] = 1\n",
        "\n",
        "\n",
        "begin_time = time.time()\n",
        "\n",
        "# Network parameters\n",
        "learning_rate = 0.002\n",
        "batch_size = 200\n",
        "training_iteration_num = 3000\n",
        "\n",
        "n_hidden_1 = 256  # 1st layer number of neurons\n",
        "n_hidden_2 = 256  # 2nd layer number of neurons\n",
        "n_hidden_3 = 256  # 2nd layer number of neurons\n",
        "n_input = data_train.shape[1]  # MNIST data input (img shape: 28*28)\n",
        "n_classes = label_train.shape[1]  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5\n",
        "\n",
        "# Deep ANN model\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def multilayer_perceptron(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "\n",
        "    # Hidden fully connected layer with 128 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    # layer_2 = tf.nn.dropout(layer_2, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.sigmoid(layer_3)\n",
        "    # layer_3 = tf.nn.dropout(layer_3, dropout)\n",
        "\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #with tf.device('/gpu:1'):\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    total_batch_train = int(data_train.shape[0] / batch_size)\n",
        "    total_batch_test = int(data_test.shape[0] / batch_size)\n",
        "\n",
        "    for iter_num in range(training_iteration_num):\n",
        "        avg_cost_test = 0.\n",
        "        avg_acc_test = 0.\n",
        "        select=rd.sample(range(data_train.shape[0]), batch_size)\n",
        "        train_x = data_train[select,:]\n",
        "        train_y = label_train[select,:]\n",
        "\n",
        "        _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
        "\n",
        "        _label_train = [np.argmax(i) for i in _logits_train]\n",
        "        _label_train_y = [np.argmax(i) for i in train_y]\n",
        "        _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
        "\n",
        "\n",
        "        for i in range(total_batch_test):\n",
        "            test_x = data_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            test_y = label_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
        "            avg_cost_test += c_test\n",
        "            _label_test = [np.argmax(i) for i in _logits_test]\n",
        "            _label_test_y = [np.argmax(i) for i in test_y]\n",
        "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
        "            avg_acc_test += _accuracy_test\n",
        "        if iter_num % 5 == 0:\n",
        "            print(\"[%5d/%d] train_loss: %.5f, train_accuracy: %.5f ; test_loss: %.5f, test_accuracy: %.5f\" %\n",
        "                  (iter_num, training_iteration_num, c_train, _accuracy_train, avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "            #print(\"Test Loss: %f, Test_acc: %f\" % (avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "\n",
        "end_time = time.time()\n",
        "complete = end_time - begin_time\n",
        "min = int(complete/60)\n",
        "secs = round(complete % 60, 2)\n",
        "\n",
        "print(\"Your program finished in %d minutes %d seconds!\" % (min, secs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0qcvay_mFRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rd\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "# Preparing the data set\n",
        "with open('AI_quick_draw.pickle', 'rb') as open_ai_quick:\n",
        "    data_train = pickle.load(open_ai_quick)\n",
        "    label_train1 = pickle.load(open_ai_quick)\n",
        "    data_test = pickle.load(open_ai_quick)\n",
        "    label_test1 = pickle.load(open_ai_quick)\n",
        "n_classes = len(np.unique(label_train1))\n",
        "# convert labels to 0-1 hot encoding\n",
        "label_train = np.zeros((label_train1.shape[0], n_classes))\n",
        "a = np.arange(label_train1.shape[0], dtype=np.int64)\n",
        "b = np.array(label_train1, dtype=np.int64).reshape((label_train1.shape[0],))\n",
        "label_train[a, b] = 1\n",
        "\n",
        "label_test = np.zeros((label_test1.shape[0], n_classes))\n",
        "c = np.arange(label_test1.shape[0], dtype=np.int64)\n",
        "d = np.array(label_test1, dtype=np.int64).reshape((label_test1.shape[0],))\n",
        "label_test[c, d] = 1\n",
        "\n",
        "\n",
        "begin_time = time.time()\n",
        "\n",
        "# Network parameters\n",
        "learning_rate = 0.002\n",
        "batch_size = 200\n",
        "training_iteration_num = 3000\n",
        "\n",
        "n_hidden_1 = 256  # 1st layer number of neurons\n",
        "n_hidden_2 = 256  # 2nd layer number of neurons\n",
        "n_hidden_3 = 256  # 2nd layer number of neurons\n",
        "n_hidden_4 = 256  # 2nd layer number of neurons\n",
        "n_hidden_5 = 256  # 2nd layer number of neurons\n",
        "n_input = data_train.shape[1]  # MNIST data input (img shape: 28*28)\n",
        "n_classes = label_train.shape[1]  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5\n",
        "\n",
        "# Deep ANN model\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
        "    'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_5, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
        "    'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def multilayer_perceptron(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "\n",
        "    # Hidden fully connected layer with 128 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    # layer_2 = tf.nn.dropout(layer_2, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.sigmoid(layer_3)\n",
        "    # layer_3 = tf.nn.dropout(layer_3, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
        "    layer_4 = tf.nn.sigmoid(layer_4)\n",
        "    # layer_4 = tf.nn.dropout(layer_4, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']), biases['b5'])\n",
        "    layer_5 = tf.nn.sigmoid(layer_5)\n",
        "    # layer_5 = tf.nn.dropout(layer_5, dropout)\n",
        "\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_5, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #with tf.device('/gpu:1'):\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    total_batch_train = int(data_train.shape[0] / batch_size)\n",
        "    total_batch_test = int(data_test.shape[0] / batch_size)\n",
        "\n",
        "    for iter_num in range(training_iteration_num):\n",
        "        avg_cost_test = 0.\n",
        "        avg_acc_test = 0.\n",
        "        select=rd.sample(range(data_train.shape[0]), batch_size)\n",
        "        train_x = data_train[select,:]\n",
        "        train_y = label_train[select,:]\n",
        "\n",
        "        _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
        "\n",
        "        _label_train = [np.argmax(i) for i in _logits_train]\n",
        "        _label_train_y = [np.argmax(i) for i in train_y]\n",
        "        _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
        "\n",
        "\n",
        "        for i in range(total_batch_test):\n",
        "            test_x = data_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            test_y = label_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
        "            avg_cost_test += c_test\n",
        "            _label_test = [np.argmax(i) for i in _logits_test]\n",
        "            _label_test_y = [np.argmax(i) for i in test_y]\n",
        "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
        "            avg_acc_test += _accuracy_test\n",
        "        if iter_num % 5 == 0:\n",
        "            print(\"[%5d/%d] train_loss: %.5f, train_accuracy: %.5f ; test_loss: %.5f, test_accuracy: %.5f\" %\n",
        "                  (iter_num, training_iteration_num, c_train, _accuracy_train, avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "            #print(\"Test Loss: %f, Test_acc: %f\" % (avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "\n",
        "end_time = time.time()\n",
        "complete = end_time - begin_time\n",
        "min = int(complete/60)\n",
        "secs = round(complete % 60, 2)\n",
        "\n",
        "print(\"Your program finished in %d minutes %d seconds!\" % (min, secs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmKW4vZBmj7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rd\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "# Preparing the data set\n",
        "with open('AI_quick_draw.pickle', 'rb') as open_ai_quick:\n",
        "    data_train = pickle.load(open_ai_quick)\n",
        "    label_train1 = pickle.load(open_ai_quick)\n",
        "    data_test = pickle.load(open_ai_quick)\n",
        "    label_test1 = pickle.load(open_ai_quick)\n",
        "n_classes = len(np.unique(label_train1))\n",
        "# convert labels to 0-1 hot encoding\n",
        "label_train = np.zeros((label_train1.shape[0], n_classes))\n",
        "a = np.arange(label_train1.shape[0], dtype=np.int64)\n",
        "b = np.array(label_train1, dtype=np.int64).reshape((label_train1.shape[0],))\n",
        "label_train[a, b] = 1\n",
        "\n",
        "label_test = np.zeros((label_test1.shape[0], n_classes))\n",
        "c = np.arange(label_test1.shape[0], dtype=np.int64)\n",
        "d = np.array(label_test1, dtype=np.int64).reshape((label_test1.shape[0],))\n",
        "label_test[c, d] = 1\n",
        "\n",
        "\n",
        "begin_time = time.time()\n",
        "\n",
        "# Network parameters\n",
        "learning_rate = 0.002\n",
        "batch_size = 200\n",
        "training_iteration_num = 3000\n",
        "\n",
        "n_hidden_1 = 256  # 1st layer number of neurons\n",
        "n_hidden_2 = 256  # 2nd layer number of neurons\n",
        "n_hidden_3 = 256  # 2nd layer number of neurons\n",
        "n_hidden_4 = 256  # 2nd layer number of neurons\n",
        "n_hidden_5 = 256  # 2nd layer number of neurons\n",
        "n_hidden_6 = 256  # 2nd layer number of neurons\n",
        "n_hidden_7 = 256  # 2nd layer number of neurons\n",
        "n_input = data_train.shape[1]  # MNIST data input (img shape: 28*28)\n",
        "n_classes = label_train.shape[1]  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5\n",
        "\n",
        "# Deep ANN model\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
        "    'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
        "    'h6': tf.Variable(tf.random_normal([n_hidden_5, n_hidden_5])),\n",
        "    'h7': tf.Variable(tf.random_normal([n_hidden_6, n_hidden_6])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_7, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
        "    'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
        "    'b6': tf.Variable(tf.random_normal([n_hidden_6])),\n",
        "    'b7': tf.Variable(tf.random_normal([n_hidden_7])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def multilayer_perceptron(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "\n",
        "    # Hidden fully connected layer with 128 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    # layer_2 = tf.nn.dropout(layer_2, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.sigmoid(layer_3)\n",
        "    # layer_3 = tf.nn.dropout(layer_3, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
        "    layer_4 = tf.nn.sigmoid(layer_4)\n",
        "    # layer_4 = tf.nn.dropout(layer_4, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']), biases['b5'])\n",
        "    layer_5 = tf.nn.sigmoid(layer_5)\n",
        "    # layer_5 = tf.nn.dropout(layer_5, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_6 = tf.add(tf.matmul(layer_5, weights['h6']), biases['b6'])\n",
        "    layer_6 = tf.nn.sigmoid(layer_6)\n",
        "    # layer_6 = tf.nn.dropout(layer_6, dropout)\n",
        "    \n",
        "    # Hidden fully connected layer with ??? neurons\n",
        "    layer_7 = tf.add(tf.matmul(layer_6, weights['h7']), biases['b7'])\n",
        "    layer_7 = tf.nn.sigmoid(layer_7)\n",
        "    # layer_7 = tf.nn.dropout(layer_7, dropout)\n",
        "\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_7, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #with tf.device('/gpu:1'):\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    total_batch_train = int(data_train.shape[0] / batch_size)\n",
        "    total_batch_test = int(data_test.shape[0] / batch_size)\n",
        "\n",
        "    for iter_num in range(training_iteration_num):\n",
        "        avg_cost_test = 0.\n",
        "        avg_acc_test = 0.\n",
        "        select=rd.sample(range(data_train.shape[0]), batch_size)\n",
        "        train_x = data_train[select,:]\n",
        "        train_y = label_train[select,:]\n",
        "\n",
        "        _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
        "\n",
        "        _label_train = [np.argmax(i) for i in _logits_train]\n",
        "        _label_train_y = [np.argmax(i) for i in train_y]\n",
        "        _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
        "\n",
        "\n",
        "        for i in range(total_batch_test):\n",
        "            test_x = data_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            test_y = label_test[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
        "            avg_cost_test += c_test\n",
        "            _label_test = [np.argmax(i) for i in _logits_test]\n",
        "            _label_test_y = [np.argmax(i) for i in test_y]\n",
        "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
        "            avg_acc_test += _accuracy_test\n",
        "        if iter_num % 5 == 0:\n",
        "            print(\"[%5d/%d] train_loss: %.5f, train_accuracy: %.5f ; test_loss: %.5f, test_accuracy: %.5f\" %\n",
        "                  (iter_num, training_iteration_num, c_train, _accuracy_train, avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "            #print(\"Test Loss: %f, Test_acc: %f\" % (avg_cost_test/total_batch_test, avg_acc_test/total_batch_test))\n",
        "\n",
        "end_time = time.time()\n",
        "complete = end_time - begin_time\n",
        "min = int(complete/60)\n",
        "secs = round(complete % 60, 2)\n",
        "\n",
        "print(\"Your program finished in %d minutes %d seconds!\" % (min, secs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}